{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthographic variation in word embeddings\n",
    "\n",
    "Word embeddings are a neural network's representation of the relationships between words, something that we have [obsessed about](http://www.lab41.org/anything2vec/) for awhile at Lab41. A network that has seen,\n",
    "say, 20 billion words in its lifetime will have a lot to say about them. We are going to talk about why word embeddings know more about _style_ than you do--at least when it comes to linguistic style.\n",
    "\n",
    "A word embedding takes the form of giant matrix; what's neat about that is that every row of the matrix represents a word as a vector of real numbers. This by itself is nothing new -- so-called \"sparse\" vector representations of words have been around for decades. \"One-hot\" vectors, which represent a word like _hello_ by taking a bunch of zeros and changing exactly one of them to 1, at a position predetermined to correspond to the word \"hello\"--are very useful, but they have an interesting property: every word is equally distant from every other word. From _hello_ to _goodbye_ to _hamburger_ to _justice_, every word in a one-hot vector space is exactly $\\sqrt 2$ units away from every other word.\n",
    "\n",
    "<table border=\"0\"><tr><td><img src=\"pictures/hellogoodbye.png\" /></td><td><img src=pictures/hamburgerjustice.png /></td></tr></table>\n",
    "\n",
    "These are all the same distance from each other!\n",
    "\n",
    "GloVe, word2vec, and related models are made up of *dense* rather than sparse vectors, meaning they use a lot of nonzero values and so there are more dimensions on which any two words could be differentiated.  With this vector you can compare words in nifty ways. Computing the cosine of the angle between two vectors gives the cosine similarity score, which maxes out at 1 if the vectors have the same direction and gets lower as the angle between the vectors increases:\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{x \\cdot y}{||x|| \\space||y||}$$\n",
    "\n",
    "## Riding English\n",
    "Using this metric you can choose an arbitrary vector and find the words closest to it, on whatever dimensions you want. This vector could be a word, or something you calculate yourself. As an example, let's look up the 10 nearest neighbors to \"English\" in a 200-dimensional GloVe embedding trained on 27 billion words from Twitter (available from the GloVe creators; the full code I used to process the pretrained vectors is available [here](https://github.com/pcallier/glove-twitter)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib2\n",
    "import pprint\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from miniglove import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad entry 38522 , GloVe components: \" [u'\\x85'] \"\n",
      "Stopping:  300000 300000\n",
      "english\n",
      "spanish\n",
      "language\n",
      "math\n",
      "french\n",
      "speaking\n",
      "class\n",
      "arabic\n",
      "exam\n",
      "essay\n"
     ]
    }
   ],
   "source": [
    "myglove = Glove()\n",
    "vocab_size=300000\n",
    "glove_folder = \"/home/pcallier/data/datasets/glove-twitter/downloads\"\n",
    "glove_path = os.path.join(glove_folder, \"glove.twitter.27B.200d.txt\")\n",
    "# Download if necessary (big)\n",
    "if not os.path.isfile(glove_path):\n",
    "    print(\"Downloading pretrained GloVe\")\n",
    "    if not os.path.isdir(glove_folder):\n",
    "        os.makedirs(glove_folder)\n",
    "    glove_zip_path=os.path.join(glove_folder,\"glove.zip\")\n",
    "    with open(glove_zip_path, \"wb\") as glove_zip:\n",
    "        glove_url = \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n",
    "        glove_data = urllib2.urlopen(glove_url)\n",
    "        glove_zip.write(glove_data.read())\n",
    "    print(\"Downloaded.\\nExtracting...\")\n",
    "    glove_zip = zipfile.ZipFile(glove_zip_path, \"r\")\n",
    "    glove_zip.extractall(glove_folder)\n",
    "    glove_zip.close()\n",
    "    print(\"Extracted\")\n",
    "myglove.load_glove(glove_path, max_entries=vocab_size, gz=False)\n",
    "near_words = [i[0] for i in myglove.get_nearest('english')]\n",
    "for wd in near_words:\n",
    "    print wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This example shows some of the diversity in relationships that a word embedding model can represent. The relationship\n",
    "between _English_ and _Spanish_ is different than the relationship between _English_ and _language_. It also points up some of the shortcomings of the model, as the kind of _English_ that is related to _math_, _class_, and _exam_ is a different word sense than the _English_ that is related to _language_ and _speaking_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of ink has already been spilled on how and why GloVe and word2vec encode semantic and syntactic content of words. What I'd like to point out is the extent to which they also encode\n",
    "*stylistic* relationships as well, even across semantically and syntactically diverse contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter data provides a nice playground for this because it plays host to many different styles and varieties of English. One variation in written style that is super fruitful is the one between \"working\" and \"workin\", as in \"I'm working hard on this overgrown book report\" vs \"I'm workin hard not to crack up right now.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, this difference can be represented as... a difference--i.e. subtraction:\n",
    "\n",
    "$$v_{in'} \\approx v_{workin}-v_{working}$$\n",
    "\n",
    "And you can add $v_{in'}$ (I do delight in subtle word2vec humor) to other _-ing_ words to get the _-in_ forms back:\n",
    "\n",
    "$$v_{goin} \\approx v_{in'}+v_{going}$$\n",
    "\n",
    "Evidence from the Twitter GloVe results, showing the nearest neighbors of $v_{in'}+v_{going}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'goin', 0.88831662665994482),\n",
      " (u'comin', 0.77613290568496363),\n",
      " (u'gonna', 0.75843861554999137),\n",
      " (u'going', 0.75744115550051516),\n",
      " (u'gone', 0.75716465566920121)]\n"
     ]
    }
   ],
   "source": [
    "myg=myglove\n",
    "pprint.pprint(myg[myg['workin']-myg['working']+myg['going']][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this trick works for more than just taking the 'g' off of words. Turns out it can make almost any word more laid-back and relatable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'better', 0.74960560902773565),\n",
      " (u'betta', 0.74875750122722051),\n",
      " (u'gon', 0.69962990087166721),\n",
      " (u'gettin', 0.69176624707648626),\n",
      " (u'aint', 0.68820858269487417)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myg[myg['workin']-myg['working']+myg['better']][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second-nearest neighbor of $v_{workin}-v_{working}+v_{better}$ is _betta_. If you're a linguist, this is cool because _-er_=>_-a_ and _-ing_=>_-in_ are different processes. This kind of vector math, in a model that knows nothing about English grammar and phonology, demonstrates that, in a sense, _betta_:_better_::_workin_:_working_.  It makes you wonder if GloVe knows anything about English that linguists and grammarians *haven't* figured out yet.\n",
    "\n",
    "\"But Patrick,\" you say, \"'better' is still at the top of the nearest neighbors list. How can I even be sure that you have gone very far at all?  Is 'betta' normally one of the nearest neighbors of 'better'? You are the most boring liar ever.\"\n",
    "\n",
    "Thanks for writing in! While I yield the latter point, you can just look and see that 'better' has some *really* boring nearest neighbors when you don't mess with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'better', 1.0000000000000002),\n",
      " (u'than', 0.8628326837717033),\n",
      " (u'think', 0.83140772783299211),\n",
      " (u'but', 0.8271108152387191),\n",
      " (u'should', 0.82638419111297556)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myg[myg[\"better\"]][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results hold for _never_, where the second neighbor of $v_{in'} + v_{never}$ is _neva_, as in \"neva say never.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'never', 0.76820980718345255),\n",
      " (u'neva', 0.75148017760913943),\n",
      " (u'aint', 0.71765577611700071),\n",
      " (u'gon', 0.68844101454285755),\n",
      " (u'kno', 0.67143423520239087)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myg[myg['workin']-myg['working']+myg['never']][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, check out how this style transformation affects a word like _hello_, whose nearest neighbors ordinarily sound like someone's grandma inviting you in for pie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'hello', 0.99999999999999967),\n",
      " (u'hey', 0.79004704935816128),\n",
      " (u'hi', 0.76735115675033327),\n",
      " (u'dear', 0.71815507273413681),\n",
      " (u'welcome', 0.69861532706782348),\n",
      " (u'morning', 0.68387741688549786),\n",
      " (u'goodbye', 0.65207510130197299),\n",
      " (u'thanks', 0.63994165327909924),\n",
      " (u'thank', 0.6307863015211308),\n",
      " (u'yes', 0.6267277876940357)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myg[myg['hello']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the vector $v_{betta}-v_{better}$, _hello_'s neighbors begin to sound more like someone you need to block on Tinder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'hello', 0.69317891346005667),\n",
      " (u'heyy', 0.59410393324734401),\n",
      " (u'hey', 0.58777962383237869),\n",
      " (u'wassup', 0.56454447734209456),\n",
      " (u'heeey', 0.54898272580485286),\n",
      " (u'helloo', 0.54826528148085985),\n",
      " (u'heey', 0.53740413715101087)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myg[myg['workin']-myg['working']+myg['hello']][:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is beginning to look more and more like translation, well--it is a _lot_ like translation. There has been a lot of research on using dense vector models for machine translation, but there are easy examples you can pull out with the Twitter GloVe vectors. Here we take the difference between _jaune_ 'yellow' and _yellow_ (this gives us a vector that means something like \"is in French\") and add it to _dog_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'jaune', 0.59636133712321482),\n",
      " (u'chien', 0.53978101024027159),\n",
      " (u'dog', 0.52195876478283232),\n",
      " (u'lapin', 0.45008049145266815),\n",
      " (u'vert', 0.42810379873608584),\n",
      " (u'singe', 0.4163984796954972),\n",
      " (u'pet', 0.41430019738637597),\n",
      " (u'pr\\xe9f\\xe9r\\xe9', 0.41164841171416805),\n",
      " (u'poussin', 0.40766867307001764),\n",
      " (u'flipper', 0.4066168237385443)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myg[myg['jaune']-myg['yellow']+myg['dog']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back _chien_ (albeit in the second slot), which means, of course, 'dog.' Nasty trick, though, is that the same result comes out when we try it with _cat_. Like, the *same* result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'jaune', 0.60934649332900459),\n",
      " (u'chien', 0.50129263425099924),\n",
      " (u'lapin', 0.47679089647578032),\n",
      " (u'cat', 0.47173153372284493),\n",
      " (u'poussin', 0.46016845213828739),\n",
      " (u'vert', 0.44008617657357857),\n",
      " (u'singe', 0.43984166150506449),\n",
      " (u'noir', 0.42799578578268194),\n",
      " (u'rire', 0.42102450976252132),\n",
      " (u't\\xeate', 0.42084313238627324)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myg[myg['jaune']-myg['yellow']+myg['cat']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/dog-and-cat.jpg\" width=300 />\n",
    "\n",
    "Unfortunately, _chat_ is more than just a French word for \"Make Pat sneeze and cry,\" so its neighborhood is a little different than you might expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'chat', 1.0000000000000002),\n",
      " (u'skype', 0.76015260467855716),\n",
      " (u'fb', 0.6996539906125141),\n",
      " (u'chats', 0.64665352211403826),\n",
      " (u'bbm', 0.63690212719119388),\n",
      " (u'twitter', 0.63678889555221718),\n",
      " (u'dm', 0.62655024532587489),\n",
      " (u'whatsapp', 0.6259950508742973),\n",
      " (u'kik', 0.61840193558276491),\n",
      " (u'facebook', 0.61717381818479344)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myg[myg['chat']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word sense disambiguation in dense vector models: also an area of active research.\n",
    "\n",
    "Machine translation and content representation are two of the main commercial applications for dense vector models of words and documents. But word vectors are exceptionally versatile. The methods that word2vec and GloVe use to encode them inevitably end up capturing all sorts of cool information, like the stylistic permutations we played with today. As algorithmic chat [re-enters the consumer landscape](http://www.forbes.com/sites/parmyolson/2016/06/07/yahoo-chat-bots-kik-weather-news-monkeys/), getting style right in NLP may soon be just as important as nailing the content, and dense vector representations may be part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'king', 0.69214193817476943),\n",
      " (u'queen', 0.65601796378599242),\n",
      " (u'woman', 0.59939595640420673),\n",
      " (u'prince', 0.55449537284821027),\n",
      " (u'princess', 0.54145840558796821),\n",
      " (u'royal', 0.53234426923594003),\n",
      " (u'mother', 0.50829932162282887),\n",
      " (u'elizabeth', 0.50362984236174746),\n",
      " (u'women', 0.47577372704148152),\n",
      " (u'lion', 0.47300935148718831)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myg[myg['king']-myg['man']+myg['woman']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
